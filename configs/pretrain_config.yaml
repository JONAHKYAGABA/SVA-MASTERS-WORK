# MIMIC-CXR VQA Pre-training Configuration
# ============================================
# Phase 1: Pre-training on B-grade data (31.2M pairs)
#
# This configuration uses the pretrain split from MIMIC-Ext-CXR-QBA
# which contains all quality grades B and above.
#
# Usage:
#   deepspeed --num_gpus=4 train_mimic_cxr.py \
#     --config configs/pretrain_config.yaml \
#     --deepspeed_config configs/deepspeed_config.json \
#     --phase pretrain

model:
  visual_backbone: "convnext_base"
  text_encoder: "emilyalsentzer/Bio_ClinicalBERT"
  visual_feature_dim: 512
  scene_graph_dim: 134  # 6 bbox + 64 region + 64 entity
  visual_embedding_dim: 646  # 134 + 512
  hidden_size: 768
  intermediate_size: 3072
  num_hidden_layers: 6
  num_attention_heads: 12
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  sim_layers: 2  # Scene-Embedded Interaction Module layers
  
  # Embedding dimensions
  num_regions: 310  # MIMIC-Ext-CXR-QBA anatomical regions
  num_entities: 237  # MIMIC-Ext-CXR-QBA finding entities
  region_embedding_dim: 64
  entity_embedding_dim: 64
  
  # Answer heads
  num_binary_classes: 2
  num_category_classes: 14  # CheXpert categories
  num_region_classes: 26
  num_severity_classes: 4
  
  max_question_length: 128
  vocab_size: 30522

data:
  # GCP Server paths
  mimic_cxr_jpg_path: "/home/brian/dataset"
  mimic_ext_cxr_qba_path: "/home/brian/scenegraphdata/physionet.org/files/mimic-ext-cxr-qba/1.0.0"
  chexpert_labels_path: "/home/brian/dataset/mimic-cxr-2.0.0-chexpert.csv.gz"
  
  # PRE-TRAINING: Use all data (quality filtering disabled for pretraining)
  # Set to "A" for fine-tuning, "all" or "" to include everything
  quality_grade: "all"
  view_filter: "frontal_only"
  question_types: null  # All question types
  
  # Use exports folder for pre-filtered data (faster loading)
  use_exports: true
  export_grade: "B_frontal"  # Uses exports/B_frontal/ if available
  
  # Cache directory for distributed training (prevents NCCL timeout)
  # Run: python scripts/prebuild_cache.py --config configs/pretrain_config.yaml
  cache_dir: ".cache/dataset_samples"
  
  # Image processing
  image_size: 224
  normalize_mean: [0.485, 0.456, 0.406]
  normalize_std: [0.229, 0.224, 0.225]

training:
  output_dir: "./checkpoints/mimic-cxr-vqa-pretrain"
  
  # Phase identifier
  phase: "pretrain"
  
  # Batch settings for 4x L4 GPUs
  # Effective batch size: 16 * 4 * 4 = 256
  batch_size_per_gpu: 16
  gradient_accumulation_steps: 4
  
  # Pretraining: 3-5 epochs
  num_epochs: 5
  
  # Optimizer
  learning_rate: 1.0e-4  # Slightly higher for pretraining
  weight_decay: 0.01
  warmup_ratio: 0.05  # 5% warmup for pretraining
  max_grad_norm: 1.0
  
  # Mixed precision
  fp16: true
  gradient_checkpointing: true
  
  # Loss weights
  vqa_loss_weight: 1.0
  chexpert_loss_weight: 0.3  # Auxiliary CheXpert supervision
  
  # Per-head loss weights (for pretraining, focus on binary)
  binary_head_weight: 1.0
  category_head_weight: 0.3
  region_head_weight: 0.3
  severity_head_weight: 0.2
  
  # Checkpointing
  logging_steps: 100
  save_steps: 10000  # Save every 10k steps for large dataset
  save_total_limit: 3
  eval_steps: 5000
  
  # DataLoader - OPTIMIZED FOR 32 WORKERS
  dataloader_num_workers: 32
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 4
  
  # Hugging Face Hub - UPDATE WITH YOUR HF USERNAME
  hub_model_id: "KYAGABA/mimic-cxr-vqa-pretrain"  # Your HF repo
  hub_private_repo: true
  push_to_hub_strategy: "checkpoint"  # Push every checkpoint
  
  seed: 42

wandb:
  enabled: true
  project: "mimic-cxr-vqa"
  entity: "kyagabajonah"  # Your wandb username
  name: "pretrain-all-data"
  group: "pretraining"
  tags: ["ssg-vqa", "mimic-cxr", "pretrain", "b-grade", "gcp-l4"]
  notes: "Pre-training on B-grade data (31.2M pairs)"
  watch_model: false  # Disable for pretraining (slow)
  watch_log_freq: 1000
  log_model: true

deepspeed:
  enabled: true
  config_path: "configs/deepspeed_config.json"
  stage: 2

