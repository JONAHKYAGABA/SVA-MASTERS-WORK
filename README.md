# SSG-VQA for MIMIC-CXR: Medical Visual Question Answering

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Scene-Graph enhanced Visual Question Answering** adapted for Chest X-Ray analysis using MIMIC-CXR-JPG and MIMIC-Ext-CXR-QBA datasets.

<p align="center">
  <img src="asset/model.png" width="800" alt="SSG-VQA Architecture"/>
</p>

## ğŸ”¬ Key Features

- **ConvNeXt-Base Visual Backbone**: Pre-trained vision model for chest X-ray feature extraction
- **Bio+ClinicalBERT Text Encoder**: Domain-specific language model for medical questions  
- **Scene Graph Integration**: 134-dimensional scene graph embeddings from MIMIC-Ext-CXR-QBA
- **Multi-Head Answer Module**: Specialized heads for Binary, Category, Region, and Severity answers
- **Multi-Task Learning**: Joint VQA + CheXpert classification training
- **Hardware Auto-Optimization**: Automatic detection and optimization for any GPU configuration

## ğŸ“ Repository Structure

```
SSG-VQA-main/
â”œâ”€â”€ ğŸ“‚ configs/                    # Configuration files
â”‚   â”œâ”€â”€ default_config.yaml        # Default training configuration
â”‚   â”œâ”€â”€ deepspeed_config.json      # DeepSpeed ZeRO-2 settings
â”‚   â””â”€â”€ mimic_cxr_config.py        # Python config dataclasses
â”‚
â”œâ”€â”€ ğŸ“‚ data/                       # Data loading & processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ mimic_cxr_dataset.py       # MIMIC-CXR VQA dataset class
â”‚
â”œâ”€â”€ ğŸ“‚ models/                     # Model architectures
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ mimic_vqa_model.py         # Complete SSG-VQA model
â”‚
â”œâ”€â”€ ğŸ“‚ training/                   # Training utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loss.py                    # Multi-task loss functions
â”‚   â””â”€â”€ metrics.py                 # Evaluation metrics
â”‚
â”œâ”€â”€ ğŸ“‚ utils/                      # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ utils.py                   # General utilities
â”‚   â””â”€â”€ hardware_utils.py          # Hardware auto-detection
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                    # Shell scripts
â”‚   â”œâ”€â”€ setup_gcp.sh               # GCP environment setup
â”‚   â”œâ”€â”€ setup.sh                   # General setup script
â”‚   â””â”€â”€ launch_distributed_training.sh  # Multi-GPU training launcher
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                       # Documentation
â”‚   â”œâ”€â”€ MIMIC_CXR_VQA_ANALYSIS.md  # Detailed methodology analysis
â”‚   â”œâ”€â”€ MULTI_GPU_TRAINING.md      # Multi-GPU training guide
â”‚   â”œâ”€â”€ TRAINING_GUIDE.md          # Step-by-step training guide
â”‚   â”œâ”€â”€ SETUP_DATA.md              # Data setup instructions
â”‚   â”œâ”€â”€ architecture_diagram.md    # Architecture details
â”‚   â”œâ”€â”€ methodology.md             # Research methodology
â”‚   â”œâ”€â”€ mimic-cxr-jpg.md           # MIMIC-CXR-JPG documentation
â”‚   â””â”€â”€ mimic-ext-cxr-qba.md       # MIMIC-Ext-CXR-QBA documentation
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                      # Unit tests
â”œâ”€â”€ ğŸ“‚ examples/                   # Example notebooks/scripts
â”œâ”€â”€ ğŸ“‚ asset/                      # Images and assets
â”‚
â”œâ”€â”€ ğŸ“œ train_mimic_cxr.py          # Main training script
â”œâ”€â”€ ğŸ“œ evaluate.py                 # Model evaluation script
â”œâ”€â”€ ğŸ“œ analyze_data.py             # Data analysis & validation
â”œâ”€â”€ ğŸ“œ setup_data.py               # Data extraction & setup
â”œâ”€â”€ ğŸ“œ requirements.txt            # Python dependencies
â”œâ”€â”€ ğŸ“œ environment.yml             # Conda environment
â””â”€â”€ ğŸ“œ LICENSE                     # MIT License
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone https://github.com/your-username/SSG-VQA-main.git
cd SSG-VQA-main

# Create conda environment
conda create -n mimic-vqa python=3.10 -y
conda activate mimic-vqa

# Install PyTorch with CUDA 12.1 (for L4/A10 GPUs)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install dependencies
pip install -r requirements.txt
```

### 2. Data Setup

```bash
# Setup data paths (modify for your environment)
export MIMIC_CXR_PATH=/path/to/mimic-cxr-jpg
export MIMIC_QA_PATH=/path/to/mimic-ext-cxr-qba

# Extract scene graph data (if zipped)
python setup_data.py --extract_all --mimic_qa_path $MIMIC_QA_PATH

# Analyze and validate data
python analyze_data.py --mimic_cxr_path $MIMIC_CXR_PATH --mimic_qa_path $MIMIC_QA_PATH
```

### 3. Training

```bash
# Option 1: Auto-optimized training (recommended)
# Automatically detects hardware and sets optimal parameters
./scripts/launch_distributed_training.sh --config configs/default_config.yaml

# Option 2: Direct Python launch
python train_mimic_cxr.py \
    --config configs/default_config.yaml \
    --mimic_cxr_path $MIMIC_CXR_PATH \
    --mimic_qa_path $MIMIC_QA_PATH

# Option 3: GCP setup (4x L4 GPUs)
./scripts/setup_gcp.sh
```

### 4. Evaluation

```bash
python evaluate.py \
    --model_path ./checkpoints/best_model \
    --config configs/default_config.yaml
```

## âš¡ Hardware Auto-Optimization

The training pipeline automatically detects your hardware and optimizes settings:

```bash 
# Check detected hardware and optimal settings
python -m utils.hardware_utils
```

**Example output for 4x NVIDIA L4:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           HARDWARE DETECTION RESULTS                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  GPUs:             4    x NVIDIA L4                                   â•‘
â•‘  GPU Memory:       24   GB per GPU (96 GB total)                      â•‘
â•‘  Optimal Settings:                                                    â•‘
â•‘    Batch per GPU:    16                                               â•‘
â•‘    Grad accumulation: 4                                               â•‘
â•‘    Effective batch:  256                                              â•‘
â•‘    DeepSpeed:        ZeRO-2                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## ğŸ“Š Model Architecture

| Component | Details |
|-----------|---------|
| **Visual Backbone** | ConvNeXt-Base (pretrained) â†’ 512-dim features |
| **Text Encoder** | Bio+ClinicalBERT â†’ 768-dim embeddings |
| **Scene Graph** | 134-dim (6 bbox + 64 region + 64 entity) |
| **Fusion** | Scene-embedded Interaction Module (SIM) |
| **Answer Heads** | Binary (2), Category (14), Region (26), Severity (4) |
| **Auxiliary** | CheXpert 14-class classification |

## ğŸ”§ Configuration

Edit `configs/default_config.yaml`:

```yaml
model:
  visual_backbone: "convnext_base"
  text_encoder: "emilyalsentzer/Bio_ClinicalBERT"
  hidden_dim: 512
  
training:
  batch_size_per_gpu: 16           # Auto-optimized by hardware
  gradient_accumulation_steps: 4   # Effective batch = 256
  learning_rate: 2.0e-5
  num_epochs: 20
  fp16: true
  gradient_checkpointing: true
  
deepspeed:
  enabled: true                    # Auto-enabled for multi-GPU
  stage: 2                         # ZeRO optimization level
```

## ğŸ“ˆ Experiment Tracking

**Weights & Biases** integration for real-time monitoring:

```bash
# Set API key (or add to ~/.env)
export WANDB_API_KEY=your_key_here

# Training will automatically log to W&B
python train_mimic_cxr.py --config configs/default_config.yaml
```

**Hugging Face Hub** for model checkpointing:

```bash
# Set token (or add to ~/.env)
export HF_TOKEN=your_token_here

# Configure in config.yaml
training:
  hub_model_id: "your-username/mimic-cxr-vqa"
```

## ğŸ“š Documentation

- **[Training Guide](docs/TRAINING_GUIDE.md)**: Complete training walkthrough
- **[Multi-GPU Training](docs/MULTI_GPU_TRAINING.md)**: Distributed training setup
- **[Data Setup](docs/SETUP_DATA.md)**: Dataset preparation
- **[Methodology](docs/methodology.md)**: Research methodology
- **[Architecture](docs/architecture_diagram.md)**: Detailed model architecture

## ğŸ”¬ Datasets

This project uses two MIMIC datasets:

| Dataset | Description | Access |
|---------|-------------|--------|
| **MIMIC-CXR-JPG** | 377,110 chest X-ray images | [PhysioNet](https://physionet.org/content/mimic-cxr-jpg/) |
| **MIMIC-Ext-CXR-QBA** | 38.7M QA pairs with scene graphs | [PhysioNet](https://physionet.org/content/mimic-ext-cxr-qba/) |

âš ï¸ **Access Requirements**: Both datasets require credentialed PhysioNet access and CITI training.

## ğŸ“ Citation

If you use this code, please cite:

```bibtex
@article{seenivasan2023ssgqa,
  title={Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer},
  author={Seenivasan, Lalithkumar and Islam, Mobarakol and others},
  journal={MICCAI},
  year={2022}
}

@article{johnson2019mimic,
  title={MIMIC-CXR-JPG: A large publicly available database of labeled chest radiographs},
  author={Johnson, Alistair EW and others},
  journal={arXiv preprint},
  year={2019}
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request
